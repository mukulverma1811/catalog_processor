{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "from text_processing import TextProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from load_config import config\n",
    "# config"
   ]
  },
  {
   "source": [
    "### Loading Raw Catalog Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(121775, 14)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "catalog = pd.read_csv(\"/Users/mukul4.verma/Documents/workspace/catalog_indexer/data/jiomart/raw/catalog_cleaned.csv\")\n",
    "catalog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'productid', 'productskuid', 'New Product Name',\n",
       "       'Quantity', 'New_level1', 'New_level2', 'Valued_level2', 'New_level3',\n",
       "       'brandname', 'orig_score', 'aggr_score', 'processed_title',\n",
       "       'processed_title_final'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "catalog.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0  productid  productskuid  \\\n",
       "0           0  100026392    10030996.0   \n",
       "1           1  100145952    10157693.0   \n",
       "2           2  100139524    10151265.0   \n",
       "3           3  100000074    10000117.0   \n",
       "4           4  100088833    10101294.0   \n",
       "\n",
       "                                    New Product Name Quantity New_level1  \\\n",
       "0                           24 mantra organic cloves     50 g     masala   \n",
       "1                                    3 ply face mask   10 pcs        NaN   \n",
       "2  command white l plastic utility hook 1 hook 2 ...       3M       home   \n",
       "3                a and w diet root beer aged vanilla   355 ml        NaN   \n",
       "4      armr 100 herbal blackberry antihangover drink    60 ml        NaN   \n",
       "\n",
       "         New_level2     Valued_level2                 New_level3  brandname  \\\n",
       "0  spices & masalas  spices & masalas               whole spices  24 Mantra   \n",
       "1             masks               NaN                 face masks    Netplay   \n",
       "2  home improvement               NaN                        NaN         3M   \n",
       "3       soft drinks       soft drinks              aerated drink      A & W   \n",
       "4               NaN               NaN  over the counter remedies       ARMR   \n",
       "\n",
       "   orig_score  aggr_score                                    processed_title  \\\n",
       "0         0.0         0.0                            24 mantra organic clove   \n",
       "1         0.0         0.0                                    3 ply face mask   \n",
       "2         0.0         0.0  command white l plastic utility hook 1 hook 2 ...   \n",
       "3         0.0         0.0                 a and w diet root beer age vanilla   \n",
       "4         0.0         0.0      armr 100 herbal blackberry antihangover drink   \n",
       "\n",
       "                           processed_title_final  \n",
       "0                           mantra organic clove  \n",
       "1                                  ply face mask  \n",
       "2  command white plastic utility hook hook strip  \n",
       "3                 and diet root beer age vanilla  \n",
       "4      armr herbal blackberry antihangover drink  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>productid</th>\n      <th>productskuid</th>\n      <th>New Product Name</th>\n      <th>Quantity</th>\n      <th>New_level1</th>\n      <th>New_level2</th>\n      <th>Valued_level2</th>\n      <th>New_level3</th>\n      <th>brandname</th>\n      <th>orig_score</th>\n      <th>aggr_score</th>\n      <th>processed_title</th>\n      <th>processed_title_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>100026392</td>\n      <td>10030996.0</td>\n      <td>24 mantra organic cloves</td>\n      <td>50 g</td>\n      <td>masala</td>\n      <td>spices &amp; masalas</td>\n      <td>spices &amp; masalas</td>\n      <td>whole spices</td>\n      <td>24 Mantra</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>24 mantra organic clove</td>\n      <td>mantra organic clove</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>100145952</td>\n      <td>10157693.0</td>\n      <td>3 ply face mask</td>\n      <td>10 pcs</td>\n      <td>NaN</td>\n      <td>masks</td>\n      <td>NaN</td>\n      <td>face masks</td>\n      <td>Netplay</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3 ply face mask</td>\n      <td>ply face mask</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>100139524</td>\n      <td>10151265.0</td>\n      <td>command white l plastic utility hook 1 hook 2 ...</td>\n      <td>3M</td>\n      <td>home</td>\n      <td>home improvement</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3M</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>command white l plastic utility hook 1 hook 2 ...</td>\n      <td>command white plastic utility hook hook strip</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>100000074</td>\n      <td>10000117.0</td>\n      <td>a and w diet root beer aged vanilla</td>\n      <td>355 ml</td>\n      <td>NaN</td>\n      <td>soft drinks</td>\n      <td>soft drinks</td>\n      <td>aerated drink</td>\n      <td>A &amp; W</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>a and w diet root beer age vanilla</td>\n      <td>and diet root beer age vanilla</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>100088833</td>\n      <td>10101294.0</td>\n      <td>armr 100 herbal blackberry antihangover drink</td>\n      <td>60 ml</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>over the counter remedies</td>\n      <td>ARMR</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>armr 100 herbal blackberry antihangover drink</td>\n      <td>armr herbal blackberry antihangover drink</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "catalog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  column  null_percentage  total null count  # unique values\n",
       "0             New_level1               79             96452               79\n",
       "1             New_level2               13             16381              452\n",
       "2          Valued_level2               48             58801              120\n",
       "3             New_level3               12             14690             1007\n",
       "4              brandname                0                 7             6844\n",
       "5  processed_title_final                0                26            94754"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column</th>\n      <th>null_percentage</th>\n      <th>total null count</th>\n      <th># unique values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>New_level1</td>\n      <td>79</td>\n      <td>96452</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>New_level2</td>\n      <td>13</td>\n      <td>16381</td>\n      <td>452</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Valued_level2</td>\n      <td>48</td>\n      <td>58801</td>\n      <td>120</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>New_level3</td>\n      <td>12</td>\n      <td>14690</td>\n      <td>1007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>brandname</td>\n      <td>0</td>\n      <td>7</td>\n      <td>6844</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>processed_title_final</td>\n      <td>0</td>\n      <td>26</td>\n      <td>94754</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "relevant_columns = ['processed_title_final', 'New_level1', 'New_level2', 'Valued_level2', 'New_level3', 'brandname']\n",
    "column_eda_df = list()\n",
    "for column in catalog.columns:\n",
    "    column_eda = dict()\n",
    "    if column in relevant_columns:\n",
    "        column_eda['column'] = column\n",
    "        column_eda['null_percentage'] = int(100*catalog[column].isnull().sum()/catalog.shape[0])\n",
    "        column_eda['total null count'] = catalog[column].isnull().sum()\n",
    "        column_eda['# unique values'] = catalog[column].nunique()\n",
    "        column_eda_df.append(column_eda)\n",
    "\n",
    "column_eda_df = pd.DataFrame(column_eda_df)\n",
    "column_eda_df"
   ]
  },
  {
   "source": [
    "### Catalog Indexer Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_config import config\n",
    "\n",
    "import pandas as pd\n",
    "from text_processing import TextProcessing\n",
    "from compute_missing_attributes import ComputeMissingAttributes\n",
    "from compute_ampersand_category import ComputeAmpersandCategory\n",
    "\n",
    "class CatalogIndexer():\n",
    "\n",
    "    def __init__(self, config, domain, catalog):\n",
    "        self.config = config\n",
    "        self.domain = domain\n",
    "        self.catalog = catalog\n",
    "        self._validate_catalog()\n",
    "\n",
    "    def _validate_catalog(self):\n",
    "        if not type(self.catalog) == pd.core.frame.DataFrame:\n",
    "            raise Exception('catalog should be a pandas dataframe')\n",
    "        if not all([input_column in self.catalog.column for input_column in config[self.domain]['input_columns']]):\n",
    "            raise Exception('input columns mentioned in the config are not present in the given catalog')\n",
    "\n",
    "    def index_catalog(self):\n",
    "        \n",
    "        if self.config[self.domain]['run_text_processor']:\n",
    "            self.text_processor = TextProcessing()\n",
    "            self.run_text_processor()\n",
    "\n",
    "        if self.config[self.domain]['run_ampersand_category']:\n",
    "            self.compute_ampersand_category = ComputeAmpersandCategory()\n",
    "            self.run_ampersand_category()\n",
    "\n",
    "        if self.config[self.domain]['run_missing_attribute']:\n",
    "            self.compute_missing_attribute = ComputeMissingAttributes()\n",
    "            self.run_missing_attribute()\n",
    "\n",
    "        if self.config[self.domain]['run_merge_attributes']:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    def run_text_processor(self, catalog):\n",
    "        pass\n",
    "\n",
    "    def run_ampersand_category(self, catalog):\n",
    "        pass\n",
    "\n",
    "    def run_missing_attribute(self, catalog):\n",
    "        self.catalog = self.compute_missing_attribute.compute(self.catalog, self.config[self.domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "type(pd.DataFrame()) == pd.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_training_data_for_missing_column(data, 'New Product Name', 'New_level1')"
   ]
  },
  {
   "source": [
    "## Classification pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data transformation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# binary relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# performance metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# model pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "# text processing library\n",
    "# from text_processing import TextProcessing\n",
    "\n",
    "# class TextProcessor(BaseEstimator):\n",
    "\n",
    "#     def __init__(self, text_preprocessing_model, text_column):\n",
    "#         self.text_column = text_column\n",
    "#         self.text_preprocessing_model = text_preprocessing_model\n",
    "\n",
    "#     def fit(self, documents, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, x_dataset):\n",
    "#         x_dataset['cleaned_text'] = x_dataset[self.text_column].apply(lambda x: self.text_preprocessing_model.clean_text(x))\n",
    "#         return x_dataset\n",
    "import sister\n",
    "\n",
    "class SentenceEmbedding():\n",
    "\n",
    "    def __init__(self, embedding_type = None):\n",
    "        # if embedding_type == 'bert':\n",
    "        #     self.sentence_embedding = sister.BertEmbedding(lang=\"en\")\n",
    "        # else:\n",
    "        #     self.sentence_embedding = sister.MeanEmbedding(lang=\"en\")\n",
    "        if embedding_type == 'bert':\n",
    "            self.sentence_embedding = sentence_embedding\n",
    "        else:\n",
    "            self.sentence_embedding = sentence_embedding_bert\n",
    "\n",
    "    def get_sentence_embedding(self, sentence):\n",
    "        if type(sentence) == str:\n",
    "            return self.sentence_embedding(sentence)\n",
    "        else:\n",
    "            return [self.sentence_embedding(str(sent)) for sent in list(sentence)]\n",
    "\n",
    "class TextVectorizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, text_column, vectorizer_algorithm, embedding_type = None):\n",
    "        self.text_column = text_column\n",
    "        self.vectorizer_algorithm = vectorizer_algorithm\n",
    "        self.vectorizer = None\n",
    "        self.sentence_embedding = SentenceEmbedding(embedding_type) if vectorizer_algorithm == 'sentence_embeddings' else None\n",
    "\n",
    "    def fit(self, x_dataset, y=None):\n",
    "        if self.vectorizer_algorithm == 'count_vectorizer':\n",
    "            pass\n",
    "        elif self.vectorizer_algorithm == 'tfidf_vectorizer':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.vectorizer.fit(x_dataset)\n",
    "        elif self.vectorizer_algorithm == 'sentence_embeddings':\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(f'invalid vectorizer_algorithm: {vectorizer_algorithm}')\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        if self.vectorizer_algorithm == 'count_vectorizer':\n",
    "            pass\n",
    "        elif self.vectorizer_algorithm == 'tfidf_vectorizer':\n",
    "            x_dataset = self.vectorizer.transform(x_dataset)\n",
    "        elif self.vectorizer_algorithm == 'sentence_embeddings':\n",
    "            self.sentence_embedding.get_sentence_embedding(x_dataset)\n",
    "        else:\n",
    "            raise Exception(f'invalid vectorizer_algorithm: {vectorizer_algorithm}')\n",
    "        return x_dataset\n",
    "\n",
    "class TextClassification():\n",
    "\n",
    "    def __init__(self, data, text_column, label_column, classification_type, model_algorithm, vectorizer_algorithm):\n",
    "        \n",
    "        # assigning params to instance variable\n",
    "        self.data = data\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.classification_type = classification_type\n",
    "        self.model_algorithm = model_algorithm\n",
    "        self.vectorizer_algorithm = vectorizer_algorithm\n",
    "        \n",
    "        # self.text_preprocessing_model = text_preprocessing_model\n",
    "\n",
    "        # process data\n",
    "        self.process_data()\n",
    "\n",
    "        #building pipeline\n",
    "        self._build_model_pipeline()\n",
    "\n",
    "    def process_data(self,):\n",
    "        \n",
    "        self.data = self.data[[self.text_column, self.label_column]]\n",
    "        \n",
    "        # transforming target label\n",
    "        if self.classification_type == 'multi-class':\n",
    "            self.encoder = LabelEncoder()\n",
    "            self.data[self.label_column] = self.encoder.fit_transform(self.data[self.label_column])\n",
    "        else:\n",
    "            self.encoder = MultiLabelBinarizer()\n",
    "            self.data[self.label_column] = self.encoder.fit_transform(self.data[self.label_column])\n",
    "\n",
    "        # splitting of training and test data\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(self.data[self.text_column], self.data[self.label_column], test_size=0.2, shuffle=True, random_state=1)\n",
    "\n",
    "    # building end to end pipeline\n",
    "    def _build_model_pipeline(self,):\n",
    "\n",
    "        pipeline_steps = list()\n",
    "        pipeline_steps.append(('text_vectorizer', TextVectorizer(self.text_column, self.vectorizer_algorithm)))\n",
    "        # pipeline_steps.append(('column_transformation', self._load_column_transformer()))\n",
    "\n",
    "        if self.model_algorithm == 'LR' or self.model_algorithm == None:\n",
    "            pipeline_steps.append(('LogisticRegression', OneVsRestClassifier(LogisticRegression())))\n",
    "\n",
    "        elif self.model_algorithm == 'NB':\n",
    "            pipeline_steps.append(('MultinomialNB', OneVsRestClassifier(MultinomialNB())))\n",
    "\n",
    "        elif self.model_algorithm == 'SVC':\n",
    "            pipeline_steps.append(('LinearSVC', OneVsRestClassifier(LinearSVC())))\n",
    "\n",
    "        elif self.model_algorithm == 'XGB':\n",
    "            pipeline_steps.append(('XGBClassifier', OneVsRestClassifier(XGBClassifier(verbosity = 0))))\n",
    "\n",
    "        self.model_pipeline = Pipeline(steps=pipeline_steps)\n",
    "\n",
    "    # pipeline component for transforming any columns in input dataframe\n",
    "    def _load_column_transformer(self,):\n",
    "        return None\n",
    "\n",
    "    # training pipeline\n",
    "    def train_pipeline(self,):\n",
    "        self.model_pipeline.fit(self.train_x, self.train_y)\n",
    "\n",
    "    # evaluating pipeline\n",
    "    def evaluate_pipeline(self,):\n",
    "        y_pred = self.model_pipeline.predict(self.test_x)\n",
    "        metric_score = f1_score(self.test_y, y_pred, average=\"micro\")\n",
    "        print(f'F1 score for {self.model_algorithm}: {metric_score}')\n",
    "\n",
    "    # prediction using trained pipeline\n",
    "    def predict(self, input_text):\n",
    "        input_text = [input_text]\n",
    "        prediction = self.model_pipeline.predict(input_text)\n",
    "        return self.encoder.inverse_transform(prediction)\n"
   ]
  },
  {
   "source": [
    "## from cleaned dataframe to training data for pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'run_spell_check': False,\n",
       " 'run_ampersand_category': False,\n",
       " 'run_missing_attribute': True,\n",
       " 'run_merge_attributes': False,\n",
       " 'columns_for_text_processing': [],\n",
       " 'columns_for_ampersand_category': [],\n",
       " 'columns_for_missing_attributes': ['New_level1',\n",
       "  'New_level2',\n",
       "  'New_level3',\n",
       "  'Valued_level2'],\n",
       " 'columns_for_input_text_data': ['processed_title_final']}"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "domain_config = config['jiomart_autosuggest']\n",
    "domain_config"
   ]
  },
  {
   "source": [
    "### training using TF-IDF vectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Column: New_level1\n",
      "78\n",
      "model_algo: LR\n",
      "F1 score for LR: 0.8966545919454507\n",
      "\n",
      "model_algo: NB\n",
      "F1 score for NB: 0.8670360110803325\n",
      "\n",
      "model_algo: SVC\n",
      "F1 score for SVC: 0.9477945876837843\n",
      "\n",
      "\n",
      "Column: New_level2\n",
      "451\n",
      "model_algo: LR\n",
      "F1 score for LR: 0.8295745400106245\n",
      "\n",
      "model_algo: NB\n",
      "F1 score for NB: 0.7698362872458588\n",
      "\n",
      "model_algo: SVC\n",
      "F1 score for SVC: 0.8566668276428261\n",
      "\n",
      "\n",
      "Column: New_level3\n",
      "1006\n",
      "model_algo: LR\n",
      "F1 score for LR: 0.6931812782133523\n",
      "\n",
      "model_algo: NB\n",
      "F1 score for NB: 0.544024708956997\n",
      "\n",
      "model_algo: SVC\n",
      "F1 score for SVC: 0.7445949156569256\n",
      "\n",
      "\n",
      "Column: Valued_level2\n",
      "119\n",
      "model_algo: LR\n",
      "F1 score for LR: 0.8834983228340014\n",
      "\n",
      "model_algo: NB\n",
      "F1 score for NB: 0.8399738198478278\n",
      "\n",
      "model_algo: SVC\n",
      "F1 score for SVC: 0.9109056696392048\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "INPUT_TEXT_COLUMN = 'input_text'\n",
    "catalog[INPUT_TEXT_COLUMN] = catalog.apply(lambda row: \" \".join(str(row[column]) for column in domain_config['columns_for_input_text_data']), axis = 1)\n",
    "classification_pipelines = dict()\n",
    "model_path = '/Users/mukul4.verma/Documents/workspace/catalog_indexer/src/models/classification_pipelines.pkl'\n",
    "\n",
    "for column in domain_config['columns_for_missing_attributes']:\n",
    "    print(f\"Column: {column}\")\n",
    "    train_df = catalog[[column, 'input_text']]\n",
    "    train_df.dropna(inplace=True)\n",
    "    train_df = train_df[~train_df[column].str.contains('dummy')]\n",
    "    print(train_df[column].nunique())\n",
    "\n",
    "    for algo in ['LR', 'NB', 'SVC']:\n",
    "        print(f'model_algo: {algo}')\n",
    "        classification = TextClassification(train_df,INPUT_TEXT_COLUMN, column, 'multi-class', algo, 'tfidf_vectorizer')\n",
    "        classification.train_pipeline()\n",
    "        classification.evaluate_pipeline()\n",
    "        classification_pipelines[f'{column}_{algo}'] = classification\n",
    "        with open(model_path, 'wb') as dbfile:\n",
    "            pickle.dump(classification_pipelines, dbfile) \n",
    "\n",
    "        print()\n",
    "    print()\n",
    "    # break\n",
    "    # print(train_df.head())\n",
    "    # print(train_df.shape)\n",
    "    # break"
   ]
  },
  {
   "source": [
    "### training using sentence embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Column: New_level1\nmodel_algo: LR\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-b281249ee819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'model_algo: {algo}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mINPUT_TEXT_COLUMN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'multi-class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence_embeddings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mclassification_pipelines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{column}_{algo}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-d2bad61c3c6d>\u001b[0m in \u001b[0;36mtrain_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# training pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# evaluating pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-d2bad61c3c6d>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, x_dataset)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mx_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer_algorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sentence_embeddings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'invalid vectorizer_algorithm: {vectorizer_algorithm}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-d2bad61c3c6d>\u001b[0m in \u001b[0;36mget_sentence_embedding\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTextVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-d2bad61c3c6d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTextVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sister/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mhasone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sister/core.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/transformers/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         )\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/transformers/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mgroup_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             layer_group_output = self.albert_layer_groups[group_idx](\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/transformers/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malbert_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malbert_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malbert_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/transformers/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_layer_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "INPUT_TEXT_COLUMN = 'input_text'\n",
    "catalog[INPUT_TEXT_COLUMN] = catalog.apply(lambda row: \" \".join(str(row[column]) for column in domain_config['columns_for_input_text_data']), axis = 1)\n",
    "classification_pipelines = dict()\n",
    "model_path = '/Users/mukul4.verma/Documents/workspace/catalog_indexer/src/models/classification_pipelines_sent_embed_bert.pkl'\n",
    "\n",
    "for column in domain_config['columns_for_missing_attributes']:\n",
    "    print(f\"Column: {column}\")\n",
    "    train_df = catalog[[column, 'input_text']]\n",
    "    train_df.dropna(inplace=True)\n",
    "\n",
    "    for algo in ['LR', 'NB', 'SVC']:\n",
    "        print(f'model_algo: {algo}')\n",
    "        classification = TextClassification(train_df,INPUT_TEXT_COLUMN, column, 'multi-class', algo, 'sentence_embeddings')\n",
    "        classification.train_pipeline()\n",
    "        classification.evaluate_pipeline()\n",
    "        classification_pipelines[f'{column}_{algo}'] = classification\n",
    "        with open(model_path, 'wb') as dbfile:\n",
    "            pickle.dump(classification_pipelines, dbfile) \n",
    "\n",
    "        print()\n",
    "    print()\n",
    "    # break\n",
    "    # print(train_df.head())\n",
    "    # print(train_df.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sister\n",
      "  Using cached sister-0.2.2-py3-none-any.whl (5.3 kB)\n",
      "Collecting mecab-python3==0.996.5\n",
      "  Using cached mecab_python3-0.996.5-cp38-cp38-macosx_10_9_x86_64.whl (13.9 MB)\n",
      "Requirement already satisfied: Janome==0.3.10 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from sister) (0.3.10)\n",
      "Requirement already satisfied: numpy==1.19.0 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from sister) (1.19.0)\n",
      "Collecting torch==1.5.1\n",
      "  Using cached torch-1.5.1-cp38-none-macosx_10_9_x86_64.whl (80.6 MB)\n",
      "Processing /Users/mukul4.verma/Library/Caches/pip/wheels/2c/67/ed/d84123843c937d7e7f5ba88a270d11036473144143355e2747/progressbar-2.5-py3-none-any.whl\n",
      "Collecting transformers==2.11.0\n",
      "  Using cached transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "Requirement already satisfied: fasttext==0.9.2 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from sister) (0.9.2)\n",
      "Requirement already satisfied: gensim==3.8.3 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from sister) (3.8.3)\n",
      "Processing /Users/mukul4.verma/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4/future-0.18.2-py3-none-any.whl\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from transformers==2.11.0->sister) (2.25.1)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp38-cp38-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from transformers==2.11.0->sister) (4.61.1)\n",
      "Requirement already satisfied: packaging in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from transformers==2.11.0->sister) (20.4)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from transformers==2.11.0->sister) (2021.4.4)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Using cached tokenizers-0.7.0-cp38-cp38-macosx_10_10_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from fasttext==0.9.2->sister) (49.2.1)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from fasttext==0.9.2->sister) (2.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from gensim==3.8.3->sister) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from gensim==3.8.3->sister) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from gensim==3.8.3->sister) (5.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from requests->transformers==2.11.0->sister) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from requests->transformers==2.11.0->sister) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from requests->transformers==2.11.0->sister) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from requests->transformers==2.11.0->sister) (1.26.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from packaging->transformers==2.11.0->sister) (2.4.7)\n",
      "Requirement already satisfied: click in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from sacremoses->transformers==2.11.0->sister) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages (from sacremoses->transformers==2.11.0->sister) (1.0.1)\n",
      "Installing collected packages: mecab-python3, future, torch, progressbar, filelock, sentencepiece, sacremoses, tokenizers, transformers, sister\n",
      "Successfully installed filelock-3.0.12 future-0.18.2 mecab-python3-0.996.5 progressbar-2.5 sacremoses-0.0.45 sentencepiece-0.1.96 sister-0.2.2 tokenizers-0.7.0 torch-1.5.1 transformers-2.11.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/Users/mukul4.verma/.local/share/virtualenvs/co-occurrence-gucGsh7Y/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install  sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model...\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import sister\n",
    " \n",
    "sentence_embedding = sister.MeanEmbedding(lang=\"en\")\n",
    "sentence_embedding_bert = sister.BertEmbedding(lang=\"en\")\n",
    " \n",
    "def get_phrase_embedding(phrase):\n",
    "    return sentence_embedding(phrase)\n",
    "\n",
    "def get_phrase_embedding_bert(phrase):\n",
    "    return sentence_embedding_bert(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "def phrase_sim(phrase_a, phrase_b, embedding_type = None):\n",
    "    if embedding_type == 'bert':\n",
    "        return cosine(get_phrase_embedding_bert(phrase_a), get_phrase_embedding_bert(phrase_b))\n",
    "    else:\n",
    "        return cosine(get_phrase_embedding(phrase_a), get_phrase_embedding(phrase_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n768\n"
     ]
    }
   ],
   "source": [
    "print(len(get_phrase_embedding(\"haldiram\")))\n",
    "print(len(get_phrase_embedding_bert(\"haldiram\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.82543105"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "phrase_sim(\"rice\", \"kohinoor basmati rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding():\n",
    "\n",
    "    def __init__(self, embedding_type = None):\n",
    "        if embedding_type == 'bert':\n",
    "            self.sentence_embedding = sister.BertEmbedding(lang=\"en\")\n",
    "        else:\n",
    "            self.sentence_embedding = sister.MeanEmbedding(lang=\"en\")\n",
    "\n",
    "    def get_sentence_embedding(self, sentence):\n",
    "        return self.sentence_embedding(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SentenceEmbedding('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "len(se.get_sentence_embedding(['rice', 'potato'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.7340733 ,  2.075869  , -0.46255904, ...,  0.50079906,\n",
       "         0.750432  , -0.11352374],\n",
       "       [ 1.0305916 ,  1.8630819 ,  0.18010414, ...,  0.41653   ,\n",
       "         0.59846777,  0.531764  ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "se.sentence_embedding(['rice', 'potato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'translate'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-c9111c737cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'potato'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sister/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sister/core.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/co-occurrence-gucGsh7Y/lib/python3.8/site-packages/sister/tokenizers.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "se.sentence_embedding(['rice', 'potato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## old rough work"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading relevant modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['APPAREL' 'FOOD GROCERY' 'FRUITS' 'HOME' 'HOME & PERSONAL CARE'\n 'LIFESTYLE' 'PROTEIN' 'STAPLES' 'VEGETABLES']\n"
     ]
    }
   ],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(data['relevant_categories'])\n",
    "print(multilabel_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition = int(0.8*data.shape[0])\n",
    "\n",
    "target_labels = multilabel_binarizer.transform(data['relevant_categories'])\n",
    "# train_labels, test_labels = target_labels[:partition], target_labels[partition:]\n",
    "\n",
    "train_query, test_query, train_labels, test_labels = train_test_split(data['cleaned_query'], target_labels, test_size=0.2, shuffle=True, random_state=1)\n",
    "\n",
    "# train_query = data['cleaned_query'].iloc[:partition].to_list()\n",
    "# test_query = data['cleaned_query'].iloc[partition:].to_list()\n",
    "\n",
    "vectorised_train_X = vectorizer.fit_transform(train_query)\n",
    "vectorised_test_X = vectorizer.transform(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10125,) (10125, 9)\n(2532,) (2532, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train_query.shape, train_labels.shape)\n",
    "print(test_query.shape, test_labels.shape)"
   ]
  },
  {
   "source": [
    "### Logistic classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binary Relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutputMultiOutputClassifier\n",
    "\n",
    "# Performance metric\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7038653366583543"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "clf = OneVsRestClassifier(lr)\n",
    "\n",
    "clf.fit(vectorised_train_X, train_labels)\n",
    "\n",
    "# make predictions for validation set\n",
    "y_pred = clf.predict(vectorised_test_X)\n",
    "\n",
    "f1_score(test_labels, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression())"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7038653366583543"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "\n",
    "lr = LogisticRegression()\n",
    "clf = MultiOutputClassifier(lr)\n",
    "\n",
    "clf.fit(vectorised_train_X, train_labels)\n",
    "\n",
    "# make predictions for validation set\n",
    "y_pred = clf.predict(vectorised_test_X)\n",
    "\n",
    "f1_score(test_labels, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LogisticRegression())"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "source": [
    "### Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7138183517681883"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nbClassifier = MultiOutputClassifier(MultinomialNB())\n",
    "nbClassifier.fit(vectorised_train_X, train_labels)\n",
    "nbPreds = nbClassifier.predict(vectorised_test_X)\n",
    "f1_score(test_labels, nbPreds, average=\"micro\")"
   ]
  },
  {
   "source": [
    "### SVC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7253632570853114"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "svmClassifier.fit(vectorised_train_X, train_labels)\n",
    "\n",
    "svmPreds = svmClassifier.predict(vectorised_test_X)\n",
    "f1_score(test_labels, svmPreds, average=\"micro\")"
   ]
  },
  {
   "source": [
    "### XG Boost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[23:57:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:34] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:34] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:36] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:36] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:57:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.603988603988604"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb_multi = OneVsRestClassifier(xgb)\n",
    "\n",
    "xgb_multi.fit(vectorised_train_X, train_labels)\n",
    "\n",
    "# make predictions for validation set\n",
    "y_pred = xgb_multi.predict(vectorised_test_X)\n",
    "\n",
    "f1_score(test_labels, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.661923242777059"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor()\n",
    "rfr_multi = OneVsRestClassifier(rfr)\n",
    "\n",
    "rfr_multi.fit(vectorised_train_X, train_labels)\n",
    "\n",
    "# make predictions for validation set\n",
    "y_pred = rfr_multi.predict(vectorised_test_X)\n",
    "\n",
    "f1_score(test_labels, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10125, 4650)"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "vectorised_train_X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Text Classification Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, os\n",
    "# sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# binary relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# performance metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# model pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "# text processing library\n",
    "from helpers.text_processing import TextProcessing\n",
    "\n",
    "class TextProcessor(BaseEstimator):\n",
    "\n",
    "    def __init__(self, text_preprocessing_model, text_column):\n",
    "        self.text_column = text_column\n",
    "        self.text_preprocessing_model = text_preprocessing_model\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        x_dataset['cleaned_text'] = x_dataset[self.text_column].apply(lambda x: self.text_preprocessing_model.clean_text(x))\n",
    "        return x_dataset\n",
    "\n",
    "\n",
    "class TextClassification():\n",
    "\n",
    "    def __init__(self, input_data_filepath, text_column, label_column, model_algorithm, classification_type, text_preprocessing_model = None):\n",
    "        \n",
    "        # assigning \n",
    "        self.classification_type = classification_type\n",
    "        self.input_data_filepath = input_data_filepath\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.text_preprocessing_model = text_preprocessing_model\n",
    "\n",
    "        # loading data\n",
    "        self.load_data()\n",
    "\n",
    "        #building pipeline\n",
    "        self._build_model_pipeline()\n",
    "\n",
    "    def _load_data(self,):\n",
    "        file_extension = self.raw_data_filepath.split(\".\")[-1]\n",
    "        try:\n",
    "            if file_extension == \".csv\":\n",
    "                self.raw_data = pd.read_csv(self.raw_data_filepath)\n",
    "\n",
    "                self.train_x = \n",
    "                self.train_y = \n",
    "                self.test_x = \n",
    "                self.test_y = \n",
    "            else:\n",
    "                raise ValueError (f'{file_extension} file type is not supported')\n",
    "        except ValueError as ve:\n",
    "            print(f'error occured while loading data: {ve}')\n",
    "        except Exception as e:\n",
    "            print(f'error occured while loading data: {e}')\n",
    "\n",
    "    # building end to end pipeline\n",
    "    def _build_model_pipeline():\n",
    "\n",
    "        pipeline_steps = list()\n",
    "        pipeline_steps.append(('text_preprocessing', TextProcessor()))\n",
    "        pipeline_steps.append(('text_vectorizer', TextVectorizer()))\n",
    "        pipeline_steps.append(('column_transformation', self._load_column_transformer()))\n",
    "\n",
    "        if self.model_algorithm == 'LR' or self.model_algorithm == None:\n",
    "            pipeline_steps.append(('LogisticRegression', LogisticRegression()))\n",
    "\n",
    "        elif self.model_algorithm == 'NB':\n",
    "            pipeline_steps.append(('MultinomialNB', MultinomialNB()))\n",
    "\n",
    "        elif self.model_algorithm == 'SVC':\n",
    "            pipeline_steps.append(('LinearSVC', LinearSVC()))\n",
    "\n",
    "        elif self.model_algorithm == 'XGB':\n",
    "            pipeline_steps.append(('XGBClassifier', XGBClassifier()))\n",
    "\n",
    "        self.model_pipeline = Pipeline(steps=pipeline_steps)\n",
    "\n",
    "    # pipeline component for transforming any columns in input dataframe\n",
    "    def _load_column_transformer(self,):\n",
    "        return None\n",
    "\n",
    "    # training pipeline\n",
    "    def train_pipeline(self,):\n",
    "        self.model_pipeline.fit(self.train_x, self.train_y)\n",
    "\n",
    "    # evaluating pipeline\n",
    "    def evaluate_pipeline(self,):\n",
    "        y_pred = self.model_pipeline.predict(self.test_x)\n",
    "        metric_score = f1_score(self.test_y, y_pred, average=\"micro\")\n",
    "        print(f'F1 score for {self.model_algorithm}: {metric_score}')\n",
    "\n",
    "    # prediction using trained pipeline\n",
    "    def predict(self, input_text):\n",
    "        input_text = [input_text]\n",
    "        return self.model_pipeline.predict(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1.89149379, -0.39847585,  1.63856893, ...,  0.58810926,\n",
       "        -0.02542177, -0.52835426],\n",
       "       [ 1.86913773, -0.56336215,  2.03411807, ..., -0.55633   ,\n",
       "        -0.27340013,  0.72129251],\n",
       "       [ 1.95259701, -2.83144572,  0.19055535, ..., -0.60957741,\n",
       "        -2.07750191, -1.75469982],\n",
       "       ...,\n",
       "       [ 1.98033054,  1.37228804,  1.67376262, ...,  2.75687956,\n",
       "        -0.20719842,  0.39299534],\n",
       "       [-1.54305631, -0.27530218,  0.38744703, ...,  0.72769107,\n",
       "         0.51983329, -1.92306657],\n",
       "       [ 1.17970389, -3.3812155 , -0.1498426 , ...,  1.24640268,\n",
       "        -4.00356845, -3.01027048]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}